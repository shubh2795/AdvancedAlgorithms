# Getting the frozen lake example to work
import gym
import numpy as np  # 1. Load Environment and Q-table structure

env = gym.make('FrozenLake8x8-v0')
Q = np.zeros([env.observation_space.n, env.action_space.n])
# env.obeservation.n, env.action_space.n gives number of states and action in env loaded# 2. Parameters of Q-leanring
eta = .628
gma = .9
epis = 5000
rev_list = []  # rewards per episode calculate# 3. Q-learning Algorithm
for i in range(epis):
    # Reset environment
    s = env.reset()
    rAll = 0
    d = False
    j = 0
    # The Q-Table learning algorithm
    while j < 99:
        env.render()
        j += 1
        # Choose action from Q table
        a = np.argmax(Q[s, :] + np.random.randn(1, env.action_space.n) * (1. / (i + 1)))
        # Get new state & reward from environment
        s1, r, d, _ = env.step(a)
        # Update Q-Table with new knowledge
        Q[s, a] = Q[s, a] + eta * (r + gma * np.max(Q[s1, :]) - Q[s, a])
        rAll += r
        s = s1
        if d == True:
            break
    rev_list.append(rAll)
    env.render()


print("Reward Sum on all episodes " + str(sum(rev_list) / epis))
print("Final Values Q-Table", Q)



#Output:
# Reward Sum on all episodes 0.0296
# Final Values Q-Table [[1.12110151e-04 1.35450577e-04 1.23481011e-04 1.40995233e-04]
#  [1.28723189e-04 1.44469613e-04 1.53392725e-04 1.60530493e-04]
#  [1.52934256e-04 1.70727830e-04 1.85463558e-04 1.56515906e-04]
#  [1.65179094e-04 2.28104018e-04 1.90151964e-04 1.75617694e-04]
#  [2.13431391e-04 2.44749430e-04 2.51932680e-04 2.29589484e-04]
#  [2.68892259e-04 2.85902672e-04 2.67147759e-04 2.42005638e-04]
#  [3.05061743e-04 3.30353014e-04 3.62767838e-04 3.35534222e-04]
#  [3.36059844e-04 3.73694932e-04 3.71444160e-04 3.40863050e-04]
#  [1.09368701e-04 1.10488732e-04 1.16094454e-04 1.31252387e-04]
#  [1.18777943e-04 1.20146577e-04 1.27999289e-04 1.19961596e-04]
#  [1.45582482e-04 1.43620842e-04 1.67334732e-04 1.27644404e-04]
#  [6.51859169e-05 1.70175857e-04 7.23026578e-05 1.39083648e-04]
#  [2.37559591e-04 2.62046097e-04 2.70266576e-04 2.45115914e-04]
#  [2.54256237e-04 2.96458402e-04 2.84015639e-04 3.00416444e-04]
#  [2.92227818e-04 2.85901000e-04 3.94971216e-04 3.25889024e-04]
#  [3.74493617e-04 3.49089957e-04 3.87994970e-04 3.49632810e-04]
#  [9.86881964e-05 1.07016017e-04 9.95961986e-05 1.12140574e-04]
#  [1.13592761e-04 1.12265902e-04 1.22047502e-04 1.19708597e-04]
#  [1.27167876e-04 8.16990586e-05 1.23382030e-04 4.35857633e-05]
#  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]
#  [1.95258956e-04 8.41720683e-07 2.51988002e-04 1.81937083e-06]
#  [1.91715188e-05 3.01069257e-04 2.35855907e-05 2.66331296e-04]
#  [3.49904159e-04 4.37189934e-04 4.60587495e-04 3.32839647e-04]
#  [4.24254276e-04 4.42451642e-04 4.90823724e-04 4.64956060e-04]
#  [9.88080953e-05 1.07397782e-04 9.19530966e-05 9.79372523e-05]
#  [9.40168202e-05 1.00058665e-04 1.06236780e-04 1.13221546e-04]
#  [7.14837635e-05 7.03293327e-05 7.86382620e-05 1.09254584e-04]
#  [5.46042531e-05 9.68728017e-05 2.56031317e-08 9.66238317e-05]
#  [1.10266906e-04 1.42767824e-04 1.67685116e-04 2.69885340e-05]
#  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]
#  [1.25239952e-04 1.29543021e-04 4.23947321e-04 2.71384894e-05]
#  [4.68622998e-04 4.34715766e-04 4.36609429e-04 3.90329601e-04]
#  [9.51383046e-05 9.97764550e-05 9.75143183e-05 9.84442700e-05]
#  [9.81128703e-05 2.79832358e-05 1.01684928e-04 1.04095598e-04]
#  [7.47190861e-05 4.26360371e-06 6.79027995e-07 6.11677511e-05]
#  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]
#  [1.73210010e-04 1.54077850e-04 1.81818954e-04 2.02450268e-04]
#  [5.74239486e-06 2.88832663e-04 2.85030666e-04 1.99711681e-04]
#  [1.74887424e-04 3.02016336e-04 7.77136013e-05 4.54719751e-04]
#  [4.52771623e-04 4.23610010e-04 4.89790798e-04 2.92237223e-04]
#  [9.39321922e-05 3.67521503e-05 3.73951868e-05 6.78189939e-06]
#  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]
#  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]
#  [0.00000000e+00 6.78657821e-05 3.67594871e-05 8.02678086e-05]
#  [2.05300565e-05 1.63438522e-04 6.90258332e-05 8.20353158e-05]
#  [2.20303438e-04 1.01269990e-04 8.53226690e-05 2.06695992e-04]
#  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]
#  [2.71785623e-04 4.13126865e-05 4.60212132e-04 1.47664895e-04]
#  [9.48312416e-05 1.88835376e-05 5.19649216e-05 4.09546884e-05]
#  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]
#  [6.33258647e-05 6.68124788e-05 3.84651322e-05 0.00000000e+00]
#  [3.20511975e-05 4.07447882e-05 0.00000000e+00 0.00000000e+00]
#  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]
#  [7.69499865e-05 1.42639347e-05 1.26236774e-04 3.23739180e-05]
#  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]
#  [0.00000000e+00 6.28623651e-01 0.00000000e+00 9.76936043e-05]
#  [1.26355938e-04 1.24518975e-04 1.30313964e-04 1.25629437e-04]
#  [1.19613399e-04 1.34952165e-04 3.68531101e-05 6.22863016e-05]
#  [5.97763834e-05 2.66721824e-05 2.20086622e-05 1.04676345e-04]
#  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]
#  [0.00000000e+00 5.84087485e-06 3.83439105e-05 0.00000000e+00]
#  [9.65859933e-05 1.75651972e-05 4.18292805e-05 7.18748600e-06]
#  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]
#  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]

